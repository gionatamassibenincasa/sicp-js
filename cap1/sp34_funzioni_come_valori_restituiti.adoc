[[sec:proc-returned-values]]
==== Funzioni come valori restituiti

//The above examples demonstrate how the ability to pass functions as arguments significantly enhances the expressive power of our programming language. We can achieve even more expressive power by creating functions whose returned values are themselves functions.
Gli esempi precedenti dimostrano come la capacità di passare funzioni come argomenti accresca in modo significativo la potenza espressiva del nostro linguaggio di programmazione. Possiamo ottenere un potere espressivo ancora maggiore creando funzioni i cui valori restituiti sono essi stessi funzioni.

//We can illustrate this idea by looking again at the fixed-point example described at the end of section [sec:proc-general-methods]. We formulated a new version of the square-root function as a fixed-point search, starting with the observation that latexmath:[$\sqrt{x}$] is a fixed-point of the function latexmath:[$y\mapsto x/y$]. Then we used average damping to make the approximations converge. Average damping is a useful general technique in itself. Namely, given a function latexmath:[$f$], we consider the function whose value at latexmath:[$x$] is equal to the average of latexmath:[$x$] and latexmath:[$f(x)$].
Possiamo illustrare questa idea guardando di nuovo l'esempio del punto fisso descritto alla fine del <<sec:proc-general-methods>>. Abbiamo formulato una nuova versione della funzione per il calcolo della radice quadrata come ricerca in punto fisso, partendo dall'osservazione che latexmath:[\sqrt{x}] è un punto fisso della funzione latexmath:[y \mapsto x / y]. Quindi abbiamo utilizzato lo smorzamento tramite media per far convergere le approssimazioni. Lo smorzamento tramite media è di per sé un'utile tecnica generale. Vale a dire, data una funzione latexmath:[f], consideriamo la funzione il cui valore in latexmath:[x] è uguale alla media di latexmath:[x] e latexmath:[f (x)].

//We can express the idea of average damping by means of the following function:
Possiamo esprimere l'idea di smorzamento medio mediante la seguente funzione:

[source,javascript]
----
function smorazmento(f) {
    return x => valor_medio(x, f(x));
}
----

//The function `average_damp` is a function that takes as its argument a function `f` and returns as its value a function (produced by the lambda expression) that, when applied to a number `x`, produces the average of `x` and `f(x)`. For example, applying `average_damp` to the `square` function produces a function whose value at some number latexmath:[$x$] is the average of latexmath:[$x$] and latexmath:[$x^2$]. Applying this resulting function to 10 returns the average of 10 and 100, or 55:footnote:[Observe that this is a combination whose operator is itself a combination. Exercise [ex:a-plus-abs-b] already demonstrated the ability to form such combinations, but that was only a toy example. Here we begin to see the real need for such combinations—when applying a function that is obtained as the value returned by a higher-order function.]
La funzione `smorazmento` è una funzione che prende come argomento una funzione `f` e restituisce come valore una funzione (prodotta dall'espressione lambda) che, quando applicata a un numero `x`, produce la media di `x` e `f (x)`. Ad esempio, l'applicazione di `smorzamento` alla funzione `quadrato` produce una funzione il cui valore per un certo numero latexmath:[x] è la media di latexmath:[x] e latexmath:[x^2]. L'applicazione di questa funzione risultante per 10 restituisce la media di 10 e 100 ossia 55: footnote:[Osserva che questa è una combinazione il cui operatore è esso stesso una combinazione. L'esercizio <<ex:a-plus-abs-b>> ha ​​già dimostrato la capacità di formare tali combinazioni, ma quello era solo un esempio giocattolo. Qui iniziamo a vedere la reale necessità di tali combinazioni, quando si applica una funzione ottenuta come valore restituito da una funzione di ordine superiore.]

[source,javascript]
----
smorzameto(quadrato)(10);
----

//Using `average_damp`, we can reformulate the square-root function as follows:
Usando `smorzamento`, possiamo riformulare la funzione radice quadrata come segue:

[source,javascript]
----
function radice_quadrata(x) {
    return punto_fisso(smorzamento(y => x / y),
                       1.0);
}
----

//Notice how this formulation makes explicit the three ideas in the method: fixed-point search, average damping, and the function latexmath:[$y\mapsto x/y$]. It is instructive to compare this formulation of the square-root method with the original version given in section [sec:sqrt]. Bear in mind that these functions express the same process, and notice how much clearer the idea becomes when we express the process in terms of these abstractions. In general, there are many ways to formulate a process as a function. Experienced programmers know how to choose process formulations that are particularly perspicuous, and where useful elements of the process are exposed as separate entities that can be reused in other applications. As a simple example of reuse, notice that the cube root of latexmath:[$x$] is a fixed point of the function latexmath:[$y\mapsto x/y^2$], so we can immediately generalize our square-root function to one that extracts cube roots:footnote:[See exercise [ex:nth-roots] for a further generalization.]
Si noti come questa formulazione rende esplicite le tre idee nel metodo: la ricerca di un punto fisso, lo smorzamento tramite media e la funzione latexmath:[y \mapsto x / y]. È istruttivo confrontare questa formulazione del metodo della radice quadrata con la versione originale fornita nel <<sec:sqrt>>. Tieni presente che queste funzioni esprimono lo stesso processo e nota quanto più chiara l'idea diventa quando esprimiamo il processo in termini di queste astrazioni. In generale, ci sono molti modi per formulare un processo come una funzione. I programmatori esperti sanno come scegliere formulazioni di processo particolarmente perspicaci e dove elementi utili del processo sono esposti come entità separate che possono essere riutilizzate in altre applicazioni. Come semplice esempio di riutilizzo, si noti che la radice cubica di latexmath:[x] è un punto fisso della funzione latexmath:[y \mapsto x / y^2 ], quindi possiamo immediatamente generalizzare la nostra funzione radice quadrata a quella che estrae radici cubiche: footnote:[Vedere l'esercizio <<ex:nth-roots>> per un'ulteriore generalizzazione.]

[source,javascript]
----
function radice_cubica(x) {
    return punto_fisso(smorzamento(y => x / quadrato(y)),
                       1.0);
}
----

[[newtons-method]]
===== Metodo di Newton

//When we first introduced the square-root function, in section [sec:sqrt], we mentioned that this was a special case of _Newton’s method_. If latexmath:[$x\mapsto g(x)$] is a differentiable function, then a solution of the equation latexmath:[$g(x)=0$] is a fixed point of the function latexmath:[$x\mapsto f(x)$] where latexmath:[\[f(x) = x - \frac{g(x)}{Dg(x)}\]] and latexmath:[$Dg(x)$] is the derivative of latexmath:[$g$] evaluated at latexmath:[$x$]. Newton’s method is the use of the fixed-point method we saw above to approximate a solution of the equation by finding a fixed point of the function latexmath:[$f$].footnote:[Elementary calculus books usually describe Newton’s method in terms of the sequence of approximations latexmath:[$x_{n+1}=x_n-g(x_n)/Dg(x_n)$]. Having language for talking about processes and using the idea of fixed points simplifies the description of the method.] For many functions latexmath:[$g$] and for sufficiently good initial guesses for latexmath:[$x$], Newton’s method converges very rapidly to a solution of latexmath:[$g(x)=0$].footnote:[Newton’s method does not always converge to an answer, but it can be shown that in favorable cases each iteration doubles the number-of-digits accuracy of the approximation to the solution. In such cases, Newton’s method will converge much more rapidly than the half-interval method.]
Quando abbiamo introdotto per la prima volta la funzione radice quadrata, nel <<sec:sqrt>>, abbiamo detto che questo era un caso speciale del metodo di _Newton_. Se latexmath:[x \mapsto g (x)] è una funzione derivabile, allora una soluzione dell'equazione latexmath:[g (x) = 0] è un punto fisso della funzione latexmath:[x \mapsto f (x)] dove

[stem]
++++
f (x) = x - \frac {g (x)} {Dg (x)}
++++

e latexmath:[Dg (x)] è il derivata di latexmath:[g] valutata in latexmath:[x]. Il metodo di Newton è l'uso del metodo del punto fisso che abbiamo visto sopra per approssimare una soluzione dell'equazione trovando un punto fisso della funzione latexmath:[f] footnote:[I libri di calcolo elementare di solito descrivono il metodo di Newton in termini di la sequenza di approssimazioni latexmath:[x_ {n + 1} = x_n-g (x_n) / Dg (x_n)]. Avere un linguaggio per parlare dei processi e usare l'idea dei punti fissi semplifica la descrizione del metodo.] Per molte funzioni latexmath:[g] e per stime iniziali sufficientemente buone per latexmath:[x], il metodo di Newton converge molto rapidamente a una soluzione di latexmath:[g (x) = 0]. footnote:[Il metodo di Newton non converge sempre a una soluzione, ma si può dimostrare che nei casi favorevoli ogni iterazione raddoppia l'accuratezza del numero di cifre dell'approssimazione alla soluzione. In questi casi, il metodo di Newton converge molto più rapidamente rispetto al metodo di bisezione.]

//In order to implement Newton’s method as a function, we must first express the idea of derivative. Note that ``derivative,'' like average damping, is something that transforms a function into another function. For instance, the derivative of the function latexmath:[$x\mapsto x^3$] is the function latexmath:[$x \mapsto 3x^2$]. In general, if latexmath:[$g$] is a function and latexmath:[$dx$] is a small number, then the derivative latexmath:[$Dg$] of latexmath:[$g$] is the function whose value at any number latexmath:[$x$] is given (in the limit of small latexmath:[$dx$]) by latexmath:[\[Dg(x) = \frac {g(x+dx) - g(x)}{dx}\]] Thus, we can express the idea of derivative (taking latexmath:[$dx$] to be, say, 0.00001) as the function
Per implementare il metodo di Newton come una funzione, dobbiamo prima esprimere l'idea di derivata. Nota che la _derivata_, come lo smorzamento tramite media, è qualcosa che trasforma una funzione in un'altra funzione. Ad esempio, la derivata della funzione latexmath:[x \mapsto x^3] è la funzione latexmath:[x \mapsto 3x^2]. In generale, se latexmath:[g] è una funzione e latexmath:[dx] è un numero piccolo, allora la derivata latexmath:[Dg] di latexmath:[g] è la funzione il cui valore in qualsiasi numero latexmath:[x] è dato (nel limite di un piccolo latexmath:[dx]) da

[stem]
++++
Dg (x) = \frac {g (x + dx) - g (x )} {dx}
++++
 
Quindi, possiamo esprimere l'idea di derivata (prendendo latexmath:[dx] come, diciamo, 0,00001) come funzione

[source,javascript]
----
function derivata(g) {
    return x => (g(x + dx) - g(x)) / dx;
}
----

//along with the declaration
insieme alla dichiarazione

[source,javascript]
----
const dx = 0.00001;
----

//Like `average_damp`, `deriv` is a function that takes a function as argument and returns a function as value. For example, to approximate the derivative of latexmath:[$x \mapsto x^3$] at 5 (whose exact value is 75) we can evaluate
Come `smorzamento`,` derivata` è una funzione che accetta una funzione come argomento e restituisce una funzione come valore. Ad esempio, per approssimare la derivata di latexmath:[x \mapsto x^3] in 5 (il cui valore esatto è 75) possiamo valutare

[source,javascript]
----
function cubo(x) { return x * x * x; }

derivata(cubo)(5);
----

//With the aid of `deriv`, we can express Newton’s method as a fixed-point process:
Con l'aiuto della `derivata`, possiamo esprimere il metodo di Newton come un processo di punto fisso:

[source,javascript]
----
function trasformata_newton(g) {
   return x => x - g(x) / derivata(g)(x);
}
function metodo_newton(g, stima) {
   return punto_fisso(trasformata_newton(g), stima);
}
----

//The `newton_transform` function expresses the formula at the beginning of this section, and `newtons_method` is readily defined in terms of this. It takes as arguments a function that computes the function for which we want to find a zero, together with an initial guess. For instance, to find the square root of latexmath:[$x$], we can use Newton’s method to find a zero of the function latexmath:[$y\mapsto y^2-x$] starting with an initial guess of 1.footnote:[For finding square roots, Newton’s method converges rapidly to the correct solution from any starting point.] This provides yet another form of the square-root function:
La funzione `trasformata_newton` esprime la formula all'inizio di questa sezione, e `metodo_newton` è facilmente definita in termini di ciò. Prende come argomenti una funzione che calcola la funzione per la quale vogliamo trovare uno zero, insieme a una stima iniziale. Ad esempio, per trovare la radice quadrata di latexmath:[x], possiamo usare il metodo di Newton per trovare uno zero della funzione latexmath:[y \mapsto y^2-x] iniziando con una stima iniziale di 1.footnote:[Per trovare le radici quadrate, il metodo di Newton converge rapidamente alla soluzione corretta da qualsiasi punto di partenza.] Questo fornisce ancora un'altra forma della funzione radice quadrata:

[source,javascript]
----
function radice_quadrata(x) {
    return metodo_newton(y => quadrato(y) - x,
                          1.0);
}
----

[[abstractions-and-first-class-functions]]
===== Abstractions and first-class functions

We’ve seen two ways to express the square-root computation as an instance of a more general method, once as a fixed-point search and once using Newton’s method. Since Newton’s method was itself expressed as a fixed-point process, we actually saw two ways to compute square roots as fixed points. Each method begins with a function and finds a fixed point of some transformation of the function. We can express this general idea itself as a function:

....
function fixed_point_of_transform(g, transform, guess) {
    return fixed_point(transform(g), guess);
}
....

This very general function takes as its arguments a function `g` that computes some function, a function that transforms `g`, and an initial guess. The returned result is a fixed point of the transformed function.

Using this abstraction, we can recast the first square-root computation from this section (where we look for a fixed point of the average-damped version of latexmath:[$y \mapsto x/y$]) as an instance of this general method:

....
function sqrt(x) {
    return fixed_point_of_transform(
               y => x / y,
               average_damp,
               1.0);
}
....

Similarly, we can express the second square-root computation from this section (an instance of Newton’s method that finds a fixed point of the Newton transform of latexmath:[$y\mapsto y^2-x$]) as

....
function sqrt(x) {
    return fixed_point_of_transform(
               y => square(y) - x,
               newton_transform,
               1.0);
}
....

We began section [sec:higher-order-procedures] with the observation that compound functions are a crucial abstraction mechanism, because they permit us to express general methods of computing as explicit elements in our programming language. Now we’ve seen how higher-order functions permit us to manipulate these general methods to create further abstractions.

As programmers, we should be alert to opportunities to identify the underlying abstractions in our programs and to build upon them and generalize them to create more powerful abstractions. This is not to say that one should always write programs in the most abstract way possible; expert programmers know how to choose the level of abstraction appropriate to their task. But it is important to be able to think in terms of these abstractions, so that we can be ready to apply them in new contexts. The significance of higher-order functions is that they enable us to represent these abstractions explicitly as elements in our programming language, so that they can be handled just like other computational elements.

In general, programming languages impose restrictions on the ways in which computational elements can be manipulated. Elements with the fewest restrictions are said to have status. Some of the ``rights and privileges'' of first-class elements are:footnote:[The notion of first-class status of programming-language elements is due to the British computer scientist Christopher Strachey (1916–1975).]

* They may be referred to using names.
* They may be passed as arguments to functions.
* They may be returned as the results of functions.
* They may be included in data structures.footnote:[We’ll see examples of this after we introduce data structures in chapter 2.]

JavaScript, unlike other common programming languages, awards functions full first-class status. This poses challenges for efficient implementation, but the resulting gain in expressive power is enormous.footnote:[The major implementation cost of first-class functions is that allowing functions to be returned as values requires reserving storage for a function’s free names even while the function is not executing. In the JavaScript implementation we will study in section [sec:mc-eval], these names are stored in the function’s environment.]

[[ex:unlabeled26]]
.Exercise
====
Declare a function `cubic` that can be used together with the `newtons_method` function in expressions of the form

....
newtons_method(cubic(a, b, c), 1);
....

to approximate zeros of the cubic latexmath:[$x^3 +ax^2 +bx +c$].
====

////
[[solution]]
==== Solution

....
function cubic(a, b, c) {
    return x => cube(x) + a * square(x) + b * x + c;
}
....
////

[[ex:unlabeled27]]
.Exercise
====
Declare a function `double` that takes a function of one argument as argument and returns a function that applies the original function twice. For example, if `inc` is a function that adds 1 to its argument, then `double(inc)` should be a function that adds 2. What value is returned by

....
double(double(double))(inc)(5);
....
====

////
[[solution-1]]
==== Solution

....
function double(f) {
    return x => f(f(x));
}
....
////

[[ex:compose]]
.Exercise
====
Let latexmath:[$f$] and latexmath:[$g$] be two one-argument functions. The _composition_ latexmath:[$f$] after latexmath:[$g$] is defined to be the function latexmath:[$x\mapsto f(g(x))$]. Declare a function `compose` that implements composition. For example, if `inc` is a function that adds 1 to its argument,

....
compose(square, inc)(6);
....
====

////
[[solution-2]]
==== Solution

....
function compose(f, g) {
    return x => f(g(x));
}
....
////

[[ex:repeated]]
.Exercise
====
If latexmath:[$f$] is a numerical function and latexmath:[$n$] is a positive integer, then we can form the latexmath:[$n$]th repeated application of latexmath:[$f$], which is defined to be the function whose value at latexmath:[$x$] is latexmath:[$f(f(\ldots(f(x))\ldots))$]. For example, if latexmath:[$f$] is the function latexmath:[$x \mapsto x+1$], then the latexmath:[$n$]th repeated application of latexmath:[$f$] is the function latexmath:[$x \mapsto x+n$]. If latexmath:[$f$] is the operation of squaring a number, then the latexmath:[$n$]th repeated application of latexmath:[$f$] is the function that raises its argument to the latexmath:[$2^n$]th power. Write a function that takes as inputs a function that computes latexmath:[$f$] and a positive integer latexmath:[$n$] and returns the function that computes the latexmath:[$n$]th repeated application of latexmath:[$f$]. Your function should be able to be used as follows:

....
repeated(square, 2)(5);
....

Hint: You may find it convenient to use `compose` from exercise <<ex:compose>>.
====

////
[[solution-3]]
==== Solution

....
function repeated(f, n) {
    return n === 0
           ? x => x
           : compose(f, repeated(f, n - 1));
}
....
////

[[ex:smooth]]
.Exercise
====
The idea of _smoothing_ a function is an important concept in signal processing. If latexmath:[$f$] is a function and latexmath:[$dx$] is some small number, then the smoothed version of latexmath:[$f$] is the function whose value at a point latexmath:[$x$] is the average of latexmath:[$f(x-dx)$], latexmath:[$f(x)$], and latexmath:[$f(x+dx)$]. Write a function `smooth` that takes as input a function that computes latexmath:[$f$] and returns a function that computes the smoothed latexmath:[$f$]. It is sometimes valuable to repeatedly smooth a function (that is, smooth the smoothed function, and so on) to obtained the _latexmath:[$n$]-fold smoothed function_. Show how to generate the latexmath:[$n$]-fold smoothed function of any given function using `smooth` and `repeated` from exercise <<ex:repeated>>.
====

////
[[solution-4]]
==== Solution

....
const dx = 0.00001;
function smooth(f) {
    return x => (f(x - dx) + f(x) + f(x + dx)) / 3;
}
function n_fold_smooth(f, n) {
    return repeated(smooth, n)(f);
....

....
}
....
////

[[ex:nth-roots]]
.Exercise
====
We saw in section [sec:proc-general-methods] that attempting to compute square roots by naively finding a fixed point of latexmath:[$y\mapsto x/y$] does not converge, and that this can be fixed by average damping. The same method works for finding cube roots as fixed points of the average-damped latexmath:[$y\mapsto x/y^2$]. Unfortunately, the process does not work for fourth roots—a single average damp is not enough to make a fixed-point search for latexmath:[$y\mapsto x/y^3$] converge. On the other hand, if we average damp twice (i.e., use the average damp of the average damp of latexmath:[$y\mapsto x/y^3$]) the fixed-point search does converge. Do some experiments to determine how many average damps are required to compute latexmath:[$n$]th roots as a fixed-point search based upon repeated average damping of latexmath:[$y\mapsto x/y^{n-1}$]. Use this to implement a simple function for computing latexmath:[$n$]th roots using `fixed_point`, `average_damp`, and the `repeated` function of exercise [ex:repeated]. Assume that any arithmetic operations you need are available as primitives.
====

////
[[solution-5]]
==== Solution

....
function nth_root(n, x) {
    return fixed_point(repeated(average_damp, 
                                math_floor(math_log2(n)))
                       (y => x / fast_expt(y, n - 1)),
                       1.0);
}
....
////

[[ex:unlabeled28]]
.Exercise
====
Several of the numerical methods described in this chapter are instances of an extremely general computational strategy known as _iterative improvement_. Iterative improvement says that, to compute something, we start with an initial guess for the answer, test if the guess is good enough, and otherwise improve the guess and continue the process using the improved guess as the new guess. Write a function `iterative_improve` that takes two functions as arguments: a method for telling whether a guess is good enough and a method for improving a guess. The function `iterative_improve` should return as its value a function that takes a guess as argument and keeps improving the guess until it is good enough. Rewrite the `sqrt` function of section [sec:sqrt] and the `fixed_point` function of section <<sec:proc-general-methods>> in terms of `iterative_improve`.
====

////
[[solution-6]]
==== Solution

....
function iterative_improve(good_enough, improve) {
    function iterate(guess) {
        return good_enough(guess)
               ? guess
               : iterate(improve(guess));
    }
....

....
    return iterate;
}
....
////

//